算法：
+分类方法――决策树和线形以及k最近邻
+朴素贝叶斯、线形回归和logistic回归
+支持向量机
+ 基于树的回归和无监督学习
+基于相似性的聚类
+分类测度表现


# --* 线形回归
-- 一种简单场景是在GPA得分和SAT得分数据的基础上，预测一个学生是否有可能被大学生本科培养计划接受？
1.py


# --* 决策树
--分类树用于将数据分为响应变量的几个类别（如：是or否、0or1、晴天or下雨...）。如果目标变量不止两个类别，可以使用C4.5。分类树适用于响应或目标变量本质上是分类型（离散）变量的情况。
--相反，当响应变量是连续型和非离散型时，需要使用回归树。回归树适用于预测而不是分类问题。
--当预测变量与响应变量间存在线性关系时，一个标准的回归树更为合适。当预测变量和响应变量间存在非线性关系时，应使用C4.5。当响应变量只有两类时，应该使用分类树算法。
--ID3.


# --* 贝叶斯理论
P(A|B) = ( P(B|A) P(A) ) / P(B)
-- 朴素贝叶斯分类器，以贝叶斯理论为基础，适用于输入变量维度较高的情况。
先验概率、后验概率。


# --* 用TextBlob构建朴素贝叶斯分类器
-- textblob是一个有趣的库，具备文本处理功能的工具集合。配备有API，能完成自然语言处理（natural language processing，NLP）任务，比如分类、名词短语提取、部分词性标注和情感分析。
--任何NLP库都需要corpora
# 一个简单的例子：useTextBlob.py
# 可以从corpus添加更多训练数据，并评估分类器的准确性： useTextBlob2.py


# --* 用wordcloud生成词云


# --* k-最近邻(k-nearest neighbor, K-NN)分类器是最容易理解的分类方法之一（特别在少许或没有数据分布的先验知识的情况下）。
一种慵懒的算法，因为是局部最优化。
优点是高准确性、对异常值不敏感以及对数据没有假设条件。
缺点是计算昂贵且需要大量内存。
# knn_fruits_classify.py


# --* logistic 回归
二分类响应变量的模型拟合方法


# --* 支持向量机（support vector machine，SVM）是可以用于回归或分类的有监督学习方法。是非线性模型的拓展。
SVM的目标是映射或发现x和y之间的一种模式，我们想要完成X(x∈X)->Y(y∈Y)的映射。
一个典型的例子是：给出一张未知的图像，判断它是老虎还是人？另外：字符识别问题。
# simple_sklearn_svm.py


# --* 主成分分析（principal component analysis，PCA）通过简单重组和转换来变换无标签数据的属性。
确定冗余度消除变量。

# --* k-均值（k-means）聚类
# 自己实现的：k-means.py
# 使用 sklearn: simple_k-means.py